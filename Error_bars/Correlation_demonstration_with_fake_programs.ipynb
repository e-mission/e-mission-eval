{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks at correlations between dataset characteristics and percent error for artificial programs\n",
    "Splits all ceo into subsets that serve as mock programs and then splits those further to calculate correlations \n",
    "between dataset characteristics and percent error for expected. \n",
    "The correlations found using each mock program as a starting point were different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage not configured, falling back to sample, default configuration\n",
      "URL not formatted, defaulting to \"Stage_database\"\n",
      "Connecting to database URL localhost\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from uuid import UUID\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/mallen2/alternate_branches/eval-compatible-server/e-mission-server')\n",
    "\n",
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.storage.decorations.trip_queries as esdtq\n",
    "import emission.core.wrapper.user as ecwu\n",
    "\n",
    "import confusion_matrix_handling as cm_handling\n",
    "from confusion_matrix_handling import MODE_MAPPING_DICT\n",
    "import get_EC\n",
    "import helper_functions as hf\n",
    "\n",
    "import sklearn.model_selection as skm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "\n",
    "import scipy\n",
    "from itertools import chain\n",
    "\n",
    "METERS_TO_MILES = 0.000621371 # 1 meter = 0.000621371 miles\n",
    "ECAR_PROPORTION = 0 #0.01 #~1% of cars on the road are electric.\n",
    "DROVE_ALONE_TO_SHARED_RIDE_RATIO = 1\n",
    "\n",
    "df_EI = pd.read_csv(r'Public_Dashboard/auxiliary_files/energy_intensity.csv') # r stands for raw string, only matters if the path is on Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r expanded_labeled_trips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import database_related_functions as drf  # all the emission server functions for this notebook are in here.\\n\\nuser_list, os_map, uuid_program_map = drf.get_participants_programs_and_operating_systems()\\n\\n# Takes ~ 1 min 45 s to 2 min 45 s on Macbook Pro for all ceo data up to May 2022.\\nexpanded_labeled_trips = drf.get_expanded_labeled_trips(user_list)\\nexpanded_labeled_trips['os'] = expanded_labeled_trips.user_id.map(os_map)\\nexpanded_labeled_trips['program'] = expanded_labeled_trips['user_id'].map(uuid_program_map)\\n\\n# Drop columns you don't need\\nexpanded_labeled_trips = expanded_labeled_trips.drop(labels = ['source', 'end_fmt_time', 'end_loc', 'raw_trip',\\n    'start_fmt_time', 'start_loc','start_local_dt_year', 'start_local_dt_month', 'start_local_dt_day',\\n    'start_local_dt_hour', 'start_local_dt_minute', 'start_local_dt_second',\\n    'start_local_dt_weekday', 'start_local_dt_timezone',\\n    'end_local_dt_year', 'end_local_dt_month', 'end_local_dt_day',\\n    'end_local_dt_hour', 'end_local_dt_minute', 'end_local_dt_second',\\n    'end_local_dt_weekday', 'end_local_dt_timezone'], axis = 1)\\n\\nexpanded_labeled_trips['distance_miles'] = expanded_labeled_trips.distance*METERS_TO_MILES\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively you could run \"Store_expanded_labeled_trips.ipynb\" first and then this line.\n",
    "# %store -r expanded_labeled_trips \n",
    "\n",
    "'''import database_related_functions as drf  # all the emission server functions for this notebook are in here.\n",
    "\n",
    "user_list, os_map, uuid_program_map = drf.get_participants_programs_and_operating_systems()\n",
    "\n",
    "# Takes ~ 1 min 45 s to 2 min 45 s on Macbook Pro for all ceo data up to May 2022.\n",
    "expanded_labeled_trips = drf.get_expanded_labeled_trips(user_list)\n",
    "expanded_labeled_trips['os'] = expanded_labeled_trips.user_id.map(os_map)\n",
    "expanded_labeled_trips['program'] = expanded_labeled_trips['user_id'].map(uuid_program_map)\n",
    "\n",
    "# Drop columns you don't need\n",
    "expanded_labeled_trips = expanded_labeled_trips.drop(labels = ['source', 'end_fmt_time', 'end_loc', 'raw_trip',\n",
    "    'start_fmt_time', 'start_loc','start_local_dt_year', 'start_local_dt_month', 'start_local_dt_day',\n",
    "    'start_local_dt_hour', 'start_local_dt_minute', 'start_local_dt_second',\n",
    "    'start_local_dt_weekday', 'start_local_dt_timezone',\n",
    "    'end_local_dt_year', 'end_local_dt_month', 'end_local_dt_day',\n",
    "    'end_local_dt_hour', 'end_local_dt_minute', 'end_local_dt_second',\n",
    "    'end_local_dt_weekday', 'end_local_dt_timezone'], axis = 1)\n",
    "\n",
    "expanded_labeled_trips['distance_miles'] = expanded_labeled_trips.distance*METERS_TO_MILES'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_dist_MCS_df = pd.read_csv(\"unit_distance_MCS.csv\").set_index(\"moment\")\n",
    "energy_dict = cm_handling.get_energy_dict(df_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping user labeled AIR trips and trips with no OS.\n",
      "Dropped 93 trips with no sensed sections.\n",
      "Here are the number of labeled trips remaining in each program dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cc          28768\n",
       "pc          17880\n",
       "fc          11744\n",
       "stage       10715\n",
       "sc           9092\n",
       "vail         6348\n",
       "4c           5262\n",
       "prepilot     2425\n",
       "Name: program, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop trips you want to exclude from analysis.\n",
    "expanded_labeled_trips = hf.drop_unwanted_trips(expanded_labeled_trips,drop_not_a_trip=False)\n",
    "\n",
    "# Find the primary mode - the sensed mode with the longest section for each trip.\n",
    "expanded_labeled_trips = hf.get_primary_modes(expanded_labeled_trips,energy_dict,MODE_MAPPING_DICT)\n",
    "\n",
    "print('Here are the number of labeled trips remaining in each program dataset:')\n",
    "expanded_labeled_trips.program.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrices and then the EI moments from those.\n",
    "android_confusion = pd.read_csv(\"android_confusion.csv\").set_index('gt_mode')\n",
    "ios_confusion = pd.read_csv(\"ios_confusion.csv\").set_index('gt_mode')\n",
    "\n",
    "android_confusion = cm_handling.collapse_confusion_matrix(android_confusion, rows_to_collapse={\"Train\": [\"Train\"]}, columns_to_collapse={})\n",
    "ios_confusion = cm_handling.collapse_confusion_matrix(ios_confusion, rows_to_collapse={\"Train\": [\"Train\"]}, columns_to_collapse={})\n",
    "\n",
    "# here I'm referring to car_load_factor the number that we divide the drove alone energy intensity by\n",
    "# for r = 1, car_load_factor is 4/3.\n",
    "sensed_car_EI = hf.find_sensed_car_energy_intensity(energy_dict, ECAR_PROPORTION, DROVE_ALONE_TO_SHARED_RIDE_RATIO)\n",
    "energy_dict.update({\"Car, sensed\": sensed_car_EI})\n",
    "expanded_labeled_trips['distance_miles'] = expanded_labeled_trips.distance*METERS_TO_MILES\n",
    "EI_length_cov = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing energy consumption for each trip.\n",
      "Using EI length covariance = 0.\n"
     ]
    }
   ],
   "source": [
    "# calculate the energy consumption for each trip before shuffling and splitting.\n",
    "android_EI_moments_df = cm_handling.get_conditional_EI_expectation_and_variance(android_confusion,energy_dict)\n",
    "ios_EI_moments_df = cm_handling.get_conditional_EI_expectation_and_variance(ios_confusion,energy_dict)\n",
    "os_EI_moments_map = {'ios': ios_EI_moments_df, 'android': android_EI_moments_df}\n",
    "energy_consumption_df = get_EC.compute_all_EC_values(expanded_labeled_trips,unit_dist_MCS_df,energy_dict,android_EI_moments_df,ios_EI_moments_df,\n",
    "    EI_length_cov, print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split All CEO into 7 fake programs and then use splits of those programs to separately estimate correlations.\n",
    "Show that the estimated correlation between percent error and dataset characteristics is roughly the same across fake programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run these definition cells\n",
    "and then the fake programs begin in the following line:  \\\n",
    "fake_programs = get_set_splits(energy_consumption_df,n_rounds=1,n_splits_per_round=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split CEO dataset many times.\n",
    "# compute the factors for each split.\n",
    "# compute the error.\n",
    "# compute the correlation between each variable in IND_VAR with DEP_VAR\n",
    "IND_VAR = ['drove_alone_2_shared_ride', 'no_sensed_ratio', 'car_like_ratio', 'e_bike_ratio', 'not_a_trip_ratio',\n",
    "           \"car_like_as_not_car\", \"e_bike_as_car\", \"e_bike_as_not_car_bike\", \n",
    "           \"non_car_2_car_user_label\", \"mispredicted_as_walk\", \"mispredicted_as_car\", 'distance_miles']\n",
    "DEP_VAR = 'error_pct_for_confusion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple crossvalidation splitters in sklearn but all of them split into one training and one test set at a time\n",
    "# If you get this error: \"object of type 'int' has no len()\", just lower the number of splits per round\n",
    "def get_set_splits(df, n_rounds = 50, n_splits_per_round=10):\n",
    "    '''\n",
    "    Splits data into n_rounds * n_splits_per_round sets.\n",
    "    n_splits_per_round controls the size of the resulting data subsets. \n",
    "    To get lots of datasets without shrinking the size too much, we use multiple rounds of splits.\n",
    "\n",
    "    Returns numpy array of arrays of data indices.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    from numpy.random import default_rng\n",
    "    large_size_splits = []\n",
    "    for round in range(n_rounds):\n",
    "        rng = default_rng()\n",
    "        trip_index = np.array(df.index.copy())\n",
    "        rng.shuffle(trip_index)\n",
    "        # print(energy_consumption_df.index, trip_index)\n",
    "\n",
    "        # splits is a list of numpy arrays of trip indices\n",
    "        splits = np.array_split(trip_index, n_splits_per_round)\n",
    "\n",
    "        large_size_splits.append(splits)\n",
    "    \n",
    "    unnested_large_size_splits = list(chain.from_iterable(large_size_splits))\n",
    "    print(f\"Subset lengths: {len(unnested_large_size_splits[0])}. Number of subsets: {len(unnested_large_size_splits)}\")\n",
    "    #print([len(s) for s in large_size_splits])\n",
    "\n",
    "    return unnested_large_size_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset_splitting_functions as dsf\n",
    "#dsf.get_splits_and_correlations(energy_consumption_df, IND_VAR, DEP_VAR, n_rounds = 50, n_splits_per_round=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several of the split results appear normally distributed about the mean of whatever the dataset used to generate the splits was.\n",
    "def get_split_results(splits):\n",
    "    \n",
    "    CAR_LIKE_MODES = ['drove_alone', 'shared_ride', 'taxi']\n",
    "    NON_CAR_MOTORIZED_MODES = ['bus', 'free_shuttle', 'train']\n",
    "    split_result_list = []\n",
    "    for s in splits:\n",
    "        ERROR_COLS = ['error_for_confusion',\n",
    "           'error_for_prediction', 'expected', 'predicted', 'user_labeled', 'distance_miles', 'distance', 'duration']\n",
    "        curr_split_trips = energy_consumption_df.loc[s]\n",
    "        curr_split_result = {'count': len(s)}\n",
    "        for e in ERROR_COLS:\n",
    "            curr_split_result[e] = curr_split_trips[e].sum()\n",
    "        curr_split_result['drove_alone_2_shared_ride'] = curr_split_trips.query('mode_confirm == \"drove_alone\"').distance.sum() / curr_split_trips.query('mode_confirm == \"shared_ride\"').distance.sum()\n",
    "        curr_split_result['no_sensed_ratio'] = curr_split_trips.query('primary_mode == \"no_sensed\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        curr_split_result['car_like_ratio'] = curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES').distance.sum() / curr_split_trips.distance.sum()        \n",
    "        curr_split_result['e_bike_ratio'] = curr_split_trips.query('mode_confirm == \"pilot_ebike\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        curr_split_result['not_a_trip_ratio'] = curr_split_trips.query('mode_confirm == \"not_a_trip\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        \n",
    "        # car_like_as_not_car: the fraction of car trips that were wrongly labeled as not car. \n",
    "        curr_split_result['car_like_as_not_car'] = curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES & primary_mode != \"car\"').distance.sum() / curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES').distance.sum()\n",
    "        curr_split_result['e_bike_as_car'] = curr_split_trips.query('mode_confirm == \"pilot_ebike\" & primary_mode == \"car\"').distance.sum() / curr_split_trips.query('mode_confirm == \"pilot_ebike\"').distance.sum()\n",
    "        curr_split_result['e_bike_as_not_car_bike'] = curr_split_trips.query('mode_confirm == \"pilot_ebike\" & primary_mode != [\"car\", \"bicycling\"]').distance.sum() / curr_split_trips.query('mode_confirm == \"pilot_ebike\"').distance.sum()\n",
    "\n",
    "        curr_split_result['non_car_2_car_user_label'] = curr_split_trips.query('mode_confirm == @NON_CAR_MOTORIZED_MODES').distance.sum() / curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES').distance.sum()\n",
    "        curr_split_result['non_car_2_car_sensed'] = curr_split_trips.query('primary_mode == [\"bus\", \"train\"]').distance.sum() / curr_split_trips.query('primary_mode == \"car\"').distance.sum()\n",
    "        curr_split_result['mispredicted_as_walk'] = curr_split_trips.query('mode_confirm != \"walk\" & primary_mode == \"walking\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        curr_split_result['mispredicted_as_car'] = curr_split_trips.query('mode_confirm != @CAR_LIKE_MODES & primary_mode == \"car\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "    \n",
    "        # if curr_split_result['drove_alone_2_shared_ride'] > 0.5:\n",
    "            # print(f\"CHECK: drove_alone %s, shared_ride %s\" % (curr_split_trips.query('mode_confirm == \"drove_alone\"').distance_miles.sum(),\n",
    "            #                                                   curr_split_trips.query('mode_confirm == \"shared_ride\"').distance_miles.sum()))\n",
    "        # print(curr_split_result)\n",
    "        # print(f\"CHECK user_labeled {energy_consumption_df.loc[s].user_labeled.sum()}\")\n",
    "        # print(f\"CHECK error_for_confusion {energy_consumption_df.loc[s].error_for_confusion.sum()}\")\n",
    "        split_result_list.append(curr_split_result)\n",
    "    split_results = pd.DataFrame(split_result_list)\n",
    "    split_results['error_pct_for_confusion'] = (split_results.error_for_confusion / split_results.user_labeled ) * 100\n",
    "    split_results['error_pct_for_prediction'] = (split_results.error_for_prediction / split_results.user_labeled) * 100\n",
    "    return split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlations(split_results, IND_VAR, DEP_VAR):\n",
    "    ind_var_correlation_df = pd.DataFrame(columns=[\"Independent Variable\", \"Correlation\", \"p-value\"])\n",
    "    for iv in IND_VAR:\n",
    "        corr, p = scipy.stats.pearsonr(split_results[iv],split_results[DEP_VAR])\n",
    "        ind_var_correlation_df = ind_var_correlation_df.append({\"Independent Variable\": iv, \"Correlation\": corr, \"p-value\": p}, ignore_index=True)\n",
    "    return ind_var_correlation_df.set_index(\"Independent Variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits_and_correlations(df,n_rounds = 50, n_splits_per_round=10):\n",
    "    df = df.copy()\n",
    "    splits = get_set_splits(df, n_rounds, n_splits_per_round)\n",
    "    split_results = get_split_results(splits)\n",
    "    ind_var_correlation_df = find_correlations(split_results, IND_VAR, DEP_VAR)\n",
    "    return ind_var_correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset lengths: 13177. Number of subsets: 7\n"
     ]
    }
   ],
   "source": [
    "fake_programs = get_set_splits(energy_consumption_df,n_rounds=1,n_splits_per_round=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset lengths: 1318. Number of subsets: 500\n",
      "Subset lengths: 1318. Number of subsets: 500\n",
      "Subset lengths: 1318. Number of subsets: 500\n",
      "Subset lengths: 1318. Number of subsets: 500\n",
      "Subset lengths: 1318. Number of subsets: 500\n",
      "Subset lengths: 1318. Number of subsets: 500\n",
      "Subset lengths: 1318. Number of subsets: 500\n"
     ]
    }
   ],
   "source": [
    "# Takes approximately 2 min 45 seconds on a Macbook Pro when using all ceo up to May 2022.\n",
    "fake_program_correlations_map = {}\n",
    "for i, program in enumerate(fake_programs):\n",
    "    fake_program_correlations_map[i] = get_splits_and_correlations(energy_consumption_df.loc[program])[\"Correlation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "{} &     0 &     1 &     2 &     3 &     4 &     5 &     6 \\\\\n",
      "Independent Variable     &       &       &       &       &       &       &       \\\\\n",
      "\\midrule\n",
      "no sensed ratio          &  0.01 &  0.76 &  0.01 & -0.33 &  0.01 &  0.46 & -0.02 \\\\\n",
      "car like ratio           & -0.31 & -0.79 & -0.45 & -0.90 & -0.55 & -0.09 & -0.24 \\\\\n",
      "car like as not car      & -0.26 & -0.03 & -0.37 & -0.32 & -0.70 & -0.37 & -0.46 \\\\\n",
      "e bike as car            &  0.17 &  0.09 &  0.19 &  0.00 &  0.16 &  0.07 &  0.16 \\\\\n",
      "non car 2 car user label & -0.00 & -0.03 &  0.02 &  0.04 &  0.09 & -0.63 & -0.04 \\\\\n",
      "mispredicted as walk     & -0.24 & -0.21 &  0.11 &  0.83 &  0.12 &  0.05 &  0.11 \\\\\n",
      "distance miles           &  0.13 &  0.58 & -0.21 &  0.67 & -0.39 & -0.20 & -0.13 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_program_correlations_df = pd.DataFrame(columns=list(range(0,6)))\n",
    "for i in fake_program_correlations_map.keys():\n",
    "    fake_program_correlations_df[i] = fake_program_correlations_map[i]\n",
    "#fake_program_correlations_df[\"variable\"] = IND_VAR\n",
    "\n",
    "# The factors that were significant when we split the full dataset.\n",
    "formerly_significant_factors = [\"no_sensed_ratio\", \"car_like_ratio\",\"car_like_as_not_car\",\"e_bike_as_car\",\"non_car_2_car_user_label\",\"mispredicted_as_walk\",\"distance_miles\"]\n",
    "#for up to may 2022: [\"car_like_ratio\", \"e_bike_ratio\",\"car_like_as_not_car\", \"e_bike_as_car\", \"mispredicted_as_walk\",\"mispredicted_as_car\",\"distance_miles\"]\n",
    "\n",
    "report_ready_df = fake_program_correlations_df.loc[formerly_significant_factors].copy()\n",
    "report_ready_df[\"Independent Variable\"] = report_ready_df.index.str.replace('_',' ')\n",
    "print(report_ready_df.set_index(\"Independent Variable\").round(2).to_latex())\n",
    "\n",
    "# reset index, round, and latex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try again with larger subsets  of the fake programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, program in enumerate(fake_programs):\n",
    "    fake_program_correlations_map[i] = get_splits_and_correlations(energy_consumption_df.loc[program], n_rounds = 100, n_splits_per_round=5)[\"Correlation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_program_correlations_df = pd.DataFrame(columns=list(range(0,6)))\n",
    "for i in fake_program_correlations_map.keys():\n",
    "    fake_program_correlations_df[i] = fake_program_correlations_map[i]\n",
    "fake_program_correlations_df.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('emission-private-eval')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73ac5b45931ab4dd3f8e07a8d0e5daf0146eed4821bf42374f6ac6fa4af28c83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
