{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate calibration of phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook retrieves the calibration results for a particular experiment and validates them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to run experiments, you only need to edit these variables. The notebook will retrieve the appropriate spec, use it to find the calibration periods from the database and validate the calibration. You probably want to publish this notebook along with your results so that others can examine it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASTORE_URL = \"http://localhost:8080\"\n",
    "AUTHOR_EMAIL = \"shankari@eecs.berkeley.edu\"\n",
    "CURR_SPEC_ID = \"sfba_trial_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup some basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import arrow\n",
    "import pandas as pd\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the ability to make calls to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data_from_server(user_label, key_list, start_ts, end_ts):\n",
    "    post_msg = {\n",
    "        \"user\": user_label,\n",
    "        \"key_list\": key_list,\n",
    "        \"start_time\": start_ts,\n",
    "        \"end_time\": end_ts\n",
    "    }\n",
    "    # print(\"About to retrieve messages using %s\" % post_msg)\n",
    "    response = requests.post(DATASTORE_URL+\"/datastreams/find_entries/timestamp\", json=post_msg)\n",
    "    # print(\"response = %s\" % response)\n",
    "    response.raise_for_status()\n",
    "    ret_list = response.json()[\"phone_data\"]\n",
    "    # print(\"Found %d entries\" % len(ret_list))\n",
    "    return ret_list\n",
    "\n",
    "def retrieve_all_data_from_server(user_label, key_list):\n",
    "    return retrieve_data_from_server(user_label, key_list, 0, arrow.get().timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the current spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spec_entry_list = retrieve_all_data_from_server(AUTHOR_EMAIL, [\"config/evaluation_spec\"])\n",
    "curr_spec_entry = None\n",
    "for s in all_spec_entry_list:\n",
    "    if s[\"data\"][\"id\"] == CURR_SPEC_ID:\n",
    "        curr_spec_entry = s\n",
    "curr_spec = curr_spec_entry[\"data\"]\n",
    "curr_spec[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all evaluation transitions within the start and end times of this spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_start_ts = curr_spec[\"start_ts\"]\n",
    "eval_end_ts = curr_spec[\"end_ts\"]\n",
    "print(\"Evaluation ran from %s -> %s\" % (arrow.get(eval_start_ts), arrow.get(eval_end_ts)))\n",
    "phone_labels = curr_spec[\"phones\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data model here is:\n",
    "\n",
    "```\n",
    "eval_transitions\n",
    "    - android\n",
    "        - ucb.sdb.android.1\n",
    "            - list of evaluation transitions\n",
    "        - ....\n",
    "    - ios\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transitions = copy.copy(phone_labels)\n",
    "for phoneOS, phone_map in eval_transitions.items():\n",
    "    print(\"Reading data for %s phones\" % phoneOS)\n",
    "    for phone_label in phone_map:\n",
    "        print(\"Loading transitions for phone %s\" % phone_label)\n",
    "        curr_phone_transitions = retrieve_data_from_server(\"ucb.sdb.ios.1\", [\"manual/evaluation_transition\"], eval_start_ts, eval_end_ts) # FIXME\n",
    "        phone_map[phone_label] = {}\n",
    "        phone_map[phone_label][\"transitions\"] = curr_phone_transitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find calibration transitions, validate and map them to ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here onwards, we will add the results of manipulation to each phone entry - e.g.\n",
    "\n",
    "```\n",
    "eval_transitions\n",
    "    - android\n",
    "        - ucb.sdb.android.1\n",
    "            - transitions (all transition entries, added in previous step)\n",
    "            - calibration_transitions (calibration transitions, will be added in this step)\n",
    "        - ....\n",
    "    - ios\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phoneOS, phone_map in eval_transitions.items():\n",
    "    print(\"Processing data for %s phones\" % phoneOS)\n",
    "    for phone_label in phone_map:\n",
    "        print(\"Processing transitions for phone %s\" % phone_label)\n",
    "        curr_phone_transitions = [t[\"data\"] for t in phone_map[phone_label][\"transitions\"]]\n",
    "        curr_calibration_transitions = [t for t in curr_phone_transitions if t[\"transition\"] in [\"START_CALIBRATION_PERIOD\", \"STOP_CALIBRATION_PERIOD\"]]\n",
    "        print(\"Filtered %d total -> %d calibration transitions \" % (len(curr_phone_transitions), len(curr_calibration_transitions)))\n",
    "        phone_map[phone_label][\"calibration_transitions\"] = curr_calibration_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ios_1_transitions = eval_transitions[\"ios\"][\"ucb.sdb.ios.1\"][\"calibration_transitions\"]\n",
    "[(t[\"transition\"], arrow.get(t[\"ts\"])) for t in ios_1_transitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We expect that transitions occur in pairs\n",
    "def transitions_to_ranges(transition_list):\n",
    "    assert len(transition_list) % 2 == 0, \"Transitions occur in pairs, so count (%d) cannot be odd\" % len(transition_list)\n",
    "    start_transitions = transition_list[::2]\n",
    "    end_transitions = transition_list[1::2]\n",
    "    range_list = []\n",
    "    for (s, e) in zip(start_transitions, end_transitions):\n",
    "        # print(\"------------------------------------- \\n %s -> \\n %s\" % (s, e))\n",
    "        assert s[\"transition\"] == \"START_CALIBRATION_PERIOD\", \"Start transition has %s transition\" % s[\"transition\"]\n",
    "        assert e[\"transition\"] == \"STOP_CALIBRATION_PERIOD\", \"Start transition has %s transition\" % s[\"transition\"]\n",
    "        assert s[\"trip_id\"] == e[\"trip_id\"], \"trip_id mismatch! %s != %s\" % (s[\"trip_id\"], e[\"trip_id\"])\n",
    "        assert e[\"ts\"] > s[\"ts\"], \"end %s is before start %s\" % (arrow.get(e[\"ts\"]), arrow.get(s[\"ts\"]))\n",
    "        for f in [\"spec_id\", \"device_manufacturer\", \"device_model\", \"device_version\"]:\n",
    "            assert s[f] == e[f], \"Field %s mismatch! %s != %s\" % (f, s[f], e[f])\n",
    "        curr_range = {\"trip_id\": s[\"trip_id\"], \"start_ts\": s[\"ts\"], \"end_ts\": e[\"ts\"], \"duration\": (e[\"ts\"] - s[\"ts\"])}\n",
    "        range_list.append(curr_range)\n",
    "    return range_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_to_ranges(eval_transitions[\"ios\"][\"ucb.sdb.ios.1\"][\"calibration_transitions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phoneOS, phone_map in eval_transitions.items():\n",
    "    print(\"Processing data for %s phones\" % phoneOS)\n",
    "    for phone_label in phone_map:\n",
    "        curr_calibration_ranges = transitions_to_ranges(phone_map[phone_label][\"calibration_transitions\"])\n",
    "        print(\"Found %d ranges for phone %s\" % (len(curr_calibration_ranges), phone_label))\n",
    "        phone_map[phone_label][\"calibration_ranges\"] = curr_calibration_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the ranges for individual phones\n",
    "\n",
    "This involves two main checks:\n",
    "- that we have at least one calibration range for each test in the spec. Note that we do not currently enforce that we have exactly one calibration range for each test, on the theory that more calibration is always good. But I am open to argument about this\n",
    "- that the settings in the calibration range are consistent with the spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_config_map = {}\n",
    "for ct in curr_spec[\"calibration_tests\"]:\n",
    "    expected_config_map[ct[\"id\"]] = ct[\"config\"][\"sensing_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current accuracy constants\n",
    "# Since we can't read these from the phone, we hardcoded them from the documentation\n",
    "# If there are validation failures, these need to be updated\n",
    "# In the future, we could upload the options from the phone (maybe the accuracy control)\n",
    "# but that seems like overkill here\n",
    "\n",
    "accuracy_options = {\n",
    "    \"android\": {\n",
    "        \"PRIORITY_HIGH_ACCURACY\": 100,\n",
    "        \"PRIORITY_BALANCED_POWER_ACCURACY\": 102,\n",
    "        \"PRIORITY_LOW_POWER\": 104,\n",
    "        \"PRIORITY_NO_POWER\": 105\n",
    "    },\n",
    "    \"ios\": {\n",
    "        \"kCLLocationAccuracyBestForNavigation\": -2,\n",
    "        \"kCLLocationAccuracyBest\": -1,\n",
    "        \"kCLLocationAccuracyNearestTenMeters\": 10,\n",
    "        \"kCLLocationAccuracyHundredMeters\": 100,\n",
    "        \"kCLLocationAccuracyKilometer\": 1000,\n",
    "        \"kCLLocationAccuracyThreeKilometers\": 3000,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt_array_idx = lambda phoneOS: 0 if phoneOS == \"android\" else 1\n",
    "\n",
    "def validate_filter(phoneOS, config_during_test, expected_config):\n",
    "    # filter checking is a bit tricky because the expected value has two possible values and the real config has two possible values\n",
    "    expected_filter = expected_config[\"filter\"]\n",
    "    if type(expected_filter) == int:\n",
    "        ev = expected_filter\n",
    "    else:\n",
    "        assert type(expected_filter) == list, \"platform specific filters should be specified in array, not %s\" % expected_filter\n",
    "        ev = expected_filter[opt_array_idx(phoneOS)]\n",
    "        \n",
    "    if phoneOS == \"android\":\n",
    "        cvf = \"filter_time\"\n",
    "    elif phoneOS == \"ios\":\n",
    "        cvf = \"filter_distance\"\n",
    "        \n",
    "    assert config_during_test[cvf] == ev, \"Field filter mismatch! %s != %s\" % (config_during_test, expected_config)\n",
    "    \n",
    "def validate_accuracy(phoneOS, config_during_test, expected_config):\n",
    "    # expected config accuracy is an array of strings [\"PRIORITY_BALANCED_POWER_ACCURACY\", \"kCLLocationAccuracyNearestTenMeters\"]\n",
    "    # so we find the string at the correct index and then map it to the value from the options\n",
    "    ev = accuracy_options[phoneOS][expected_config[\"accuracy\"][opt_array_idx(phoneOS)]]\n",
    "    assert config_during_test[\"accuracy\"] == ev, \"Field accuracy mismatch! %s != %s\" % (config_during_test[accuracy], ev)\n",
    "\n",
    "for phoneOS, phone_map in eval_transitions.items():\n",
    "    print(\"Processing data for %s phones\" % phoneOS)\n",
    "    for phone_label in phone_map:\n",
    "        curr_calibration_ranges = phone_map[phone_label][\"calibration_ranges\"]\n",
    "        all_test_ids = [r[\"trip_id\"] for r in curr_calibration_ranges]\n",
    "        unique_test_ids = sorted(list(set(all_test_ids)))\n",
    "        spec_test_ids = sorted([ct[\"id\"] for ct in curr_spec[\"calibration_tests\"]])\n",
    "        assert unique_test_ids == spec_test_ids, \"Missing calibration test while comparing %s, %s\" % (unique_test_ids, spec_test_ids)\n",
    "        for r in curr_calibration_ranges:\n",
    "            config_during_test_entries = retrieve_data_from_server(\"ucb.sdb.ios.1\", [\"config/sensor_config\"], r[\"start_ts\"], r[\"end_ts\"]) # CHANGEME\n",
    "            assert len(config_during_test_entries) == 1, \"Out of band configuration? Found %d config changes\" % len(config_during_test_entries)\n",
    "            config_during_test = config_during_test_entries[0][\"data\"]\n",
    "            expected_config = expected_config_map[r[\"trip_id\"]]\n",
    "            # print(config_during_test, expected_config)\n",
    "            validate_filter(\"ios\", config_during_test, expected_config) # CHANGEME\n",
    "            validate_accuracy(\"ios\", config_during_test, expected_config) # CHANGEME\n",
    "            for f in expected_config:\n",
    "                if f != \"accuracy\" and f != \"filter\":\n",
    "                    assert config_during_test[f] == expected_config[f], \"Field %s mismatch! %s != %s\" % (f, config_during_test[f], expected_config[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate ranges across phones\n",
    "\n",
    "This effectively has one test right now - is the duration of the tests across phones consistent?\n",
    "TODO: We should add a reasonable fuzz factor based on real calibration.\n",
    "\n",
    "We are going to create a pandas dataframe with the following structure\n",
    "\n",
    "```\n",
    "                    android_<phone_1> android_<phone_2> android_<phone_3> ....\n",
    "<trip_id_1>\n",
    "<trip_id_2>\n",
    "...\n",
    "```\n",
    "\n",
    "Then, we can transpose it to get\n",
    "\n",
    "```\n",
    "                    <trip_id_1> <trip_id_2> <trip_id_3> ....\n",
    "android_<phone_1>\n",
    "android_<phone_2>\n",
    "...\n",
    "```\n",
    "\n",
    "then, we can get a series of durations for each `trip_id` as a series and compare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_map = {}\n",
    "for phoneOS, phone_map in eval_transitions.items():\n",
    "    print(\"Processing data for %s phones\" % phoneOS)\n",
    "    for phone_label in phone_map:\n",
    "        curr_phone_duration_map = {}\n",
    "        curr_calibration_ranges = phone_map[phone_label][\"calibration_ranges\"]\n",
    "        for r in curr_calibration_ranges:\n",
    "            curr_phone_duration_map[r[\"trip_id\"]] = r[\"duration\"]\n",
    "        duration_map[phoneOS+\"_\"+phone_label] = curr_phone_duration_map\n",
    "        \n",
    "duration_df = pd.DataFrame(duration_map).transpose()\n",
    "duration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_df.high_accuracy - duration_df.high_accuracy.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
