{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env DB_HOST=mongodb://localhost/openpath_stage\n",
    "import emission.core.get_database as edb\n",
    "import emission.storage.timeseries.aggregate_timeseries as esta\n",
    "import emission.storage.timeseries.builtin_timeseries as estb\n",
    "import emission.core.get_database as gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_docs_cursor = gdb.get_timeseries_db().find({\n",
    "    \"metadata.key\": \"stats/pipeline_time\",\n",
    "})\n",
    "\n",
    "# Display a sample of the documents\n",
    "import pprint\n",
    "pipeline_docs = list(pipeline_docs_cursor)\n",
    "if pipeline_docs:\n",
    "    single_doc = pipeline_docs[0]\n",
    "    print(\"Single Document:\")\n",
    "    pprint.pprint(single_doc)\n",
    "else:\n",
    "    print(\"No documents found for 'stats/pipeline_time'.\")\n",
    "\n",
    "# Fetch multiple documents\n",
    "pipeline_docs_sample = pipeline_docs[:5]  # Get first 5 documents\n",
    "print(\"\\nMultiple Documents:\")\n",
    "for doc in pipeline_docs_sample:\n",
    "    pprint.pprint(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "df = pd.json_normalize(pipeline_docs)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = df['data.name'].unique()\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YES I KNOW WE CAN USE `estt.TimeQuery`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Filter for rows where data.name is \"USERCACHE\"\n",
    "usercache_df = df[df['data.name'] == \"USERCACHE\"]\n",
    "\n",
    "# Step 2: Convert metadata.write_ts to datetime\n",
    "usercache_df['datetime'] = pd.to_datetime(usercache_df['metadata.write_ts'], unit='s')\n",
    "\n",
    "# Step 3: Define the start date for filtering\n",
    "start_date = pd.Timestamp('2024-11-08')  # Adjust as needed\n",
    "\n",
    "# Step 4: Filter for rows since the start date\n",
    "usercache_df = usercache_df[usercache_df['datetime'] >= start_date]\n",
    "\n",
    "# Step 5: Group by hour and count executions\n",
    "hourly_execution_counts = usercache_df.groupby(usercache_df['datetime'].dt.floor('H')).size()\n",
    "\n",
    "# Step 6: Output the results\n",
    "if hourly_execution_counts.empty:\n",
    "    print(\"No executions of 'USERCACHE' since November 8.\")\n",
    "else:\n",
    "    print(\"Hourly execution counts since November 8:\")\n",
    "    print(hourly_execution_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Step 0: Define the list of 'data.name' entries to exclude\n",
    "# These are the 'Parent' functions\n",
    "exclude_data_names = [\n",
    "    'TRIP_SEGMENTATION/segment_into_trips',\n",
    "    'TRIP_SEGMENTATION/segment_into_trips_dist/loop'\n",
    "]\n",
    "\n",
    "# Step 1: Filter for function-level data only (entries with slashes in 'data.name') and exclude specified names\n",
    "function_level_df = df[\n",
    "    df['data.name'].str.contains('/') &\n",
    "    ~df['data.name'].isin(exclude_data_names)\n",
    "].copy()\n",
    "\n",
    "# Step 2: Select the relevant columns\n",
    "selected_columns = function_level_df[['data.reading', 'data.name']].copy()\n",
    "\n",
    "# Step 3: Data Cleaning\n",
    "# Drop rows with missing values in 'data.reading' or 'data.name'\n",
    "selected_columns.dropna(subset=['data.reading', 'data.name'], inplace=True)\n",
    "\n",
    "# Ensure 'data.reading' is numeric\n",
    "selected_columns = selected_columns[pd.to_numeric(selected_columns['data.reading'], errors='coerce').notnull()]\n",
    "\n",
    "\n",
    "# Step 5: Aggregate 'data.reading' by 'data.name'\n",
    "\n",
    "# Aggregation Using Sum\n",
    "aggregated_sum = selected_columns.groupby('data.name', as_index=False)['data.reading'].sum()\n",
    "aggregated_sum.rename(columns={'data.reading': 'total_reading'}, inplace=True)\n",
    "\n",
    "# Aggregation Using Mean\n",
    "aggregated_mean = selected_columns.groupby('data.name', as_index=False)['data.reading'].mean()\n",
    "aggregated_mean.rename(columns={'data.reading': 'average_reading'}, inplace=True)\n",
    "\n",
    "# Step 6: Determine the 80th percentile threshold based on aggregated values\n",
    "\n",
    "# For Sum Aggregation\n",
    "threshold_sum = aggregated_sum['total_reading'].quantile(0.80)\n",
    "\n",
    "# For Mean Aggregation\n",
    "threshold_mean = aggregated_mean['average_reading'].quantile(0.80)\n",
    "\n",
    "# For Total Aggregation\n",
    "threshold_total = selected_columns['data.reading'].quantile(0.80)\n",
    "\n",
    "# Step 7: Split the DataFrame into top 20% and bottom 80% based on aggregated values\n",
    "\n",
    "# Using Sum Aggregation\n",
    "top20_sum = aggregated_sum[aggregated_sum['total_reading'] >= threshold_sum].sort_values(by='total_reading', ascending=False)\n",
    "bottom80_sum = aggregated_sum[aggregated_sum['total_reading'] < threshold_sum].sort_values(by='total_reading', ascending=False)\n",
    "top20_total = selected_columns[selected_columns['data.reading'] >= threshold_total].sort_values(by='data.reading', ascending=False)\n",
    "bottom80_total = selected_columns[selected_columns['data.reading'] < threshold_total].sort_values(by='data.reading', ascending=False)\n",
    "\n",
    "# Using Mean Aggregation\n",
    "top20_mean = aggregated_mean[aggregated_mean['average_reading'] >= threshold_mean].sort_values(by='average_reading', ascending=False)\n",
    "bottom80_mean = aggregated_mean[aggregated_mean['average_reading'] < threshold_mean].sort_values(by='average_reading', ascending=False)\n",
    "\n",
    "# Step 8: Define the base directory and file paths\n",
    "base_dir = os.getcwd()  # Current working directory\n",
    "\n",
    "# Paths for Sum Aggregation\n",
    "aggregated_sum_path = os.path.join(base_dir, 'aggregated_sum_function_level.csv')\n",
    "top20_sum_path = os.path.join(base_dir, 'top20_function_level_sum_sorted.csv')\n",
    "bottom80_sum_path = os.path.join(base_dir, 'bottom80_function_level_sum_sorted.csv')\n",
    "top20_total_path = os.path.join(base_dir, 'top20_function_level_sum_sorted.csv')\n",
    "bottom80_total_path = os.path.join(base_dir, 'bottm80_function_level_sum_sorted.csv')\n",
    "\n",
    "# Paths for Mean Aggregation\n",
    "aggregated_mean_path = os.path.join(base_dir, 'aggregated_mean_function_level.csv')\n",
    "top20_mean_path = os.path.join(base_dir, 'top20_function_level_mean_sorted.csv')\n",
    "bottom80_mean_path = os.path.join(base_dir, 'bottom80_function_level_mean_sorted.csv')\n",
    "\n",
    "# Step 9: Save the aggregated and categorized DataFrames to CSV files\n",
    "\n",
    "# Saving Sum Aggregation\n",
    "aggregated_sum.to_csv(aggregated_sum_path, index=False)\n",
    "top20_sum.to_csv(top20_sum_path, index=False)\n",
    "bottom80_sum.to_csv(bottom80_sum_path, index=False)\n",
    "top20_total.to_csv(top20_total_path, index=False)\n",
    "bottom80_total.to_csv(bottom80_total_path, index=False)\n",
    "\n",
    "print(f\"Aggregated Sum Function-Level Data saved to {aggregated_sum_path}\")\n",
    "print(f\"Top 20% (Sum) function-level data saved to {top20_sum_path}\")\n",
    "print(f\"Bottom 80% (Sum) function-level data saved to {bottom80_sum_path}\")\n",
    "print(f\"Top 20%  function-level data saved to {top20_total_path}\")\n",
    "print(f\"Bottom 80% function-level data saved to {bottom80_total_path}\")\n",
    "\n",
    "# Saving Mean Aggregation\n",
    "aggregated_mean.to_csv(aggregated_mean_path, index=False)\n",
    "top20_mean.to_csv(top20_mean_path, index=False)\n",
    "bottom80_mean.to_csv(bottom80_mean_path, index=False)\n",
    "\n",
    "print(f\"\\nAggregated Mean Function-Level Data saved to {aggregated_mean_path}\")\n",
    "print(f\"Top 20% (Mean) function-level data saved to {top20_mean_path}\")\n",
    "print(f\"Bottom 80% (Mean) function-level data saved to {bottom80_mean_path}\")\n",
    "\n",
    "# Step 10: Verify the splits\n",
    "print(f\"\\nSum Aggregation - Top 20% row count: {len(top20_sum)}\")\n",
    "print(f\"Sum Aggregation - Bottom 80% row count: {len(bottom80_sum)}\")\n",
    "\n",
    "print(f\"\\nMean Aggregation - Top 20% row count: {len(top20_mean)}\")\n",
    "print(f\"Mean Aggregation - Bottom 80% row count: {len(bottom80_mean)}\")\n",
    "\n",
    "# Step 11: Inspect some entries\n",
    "print(\"\\nSample Top 20% Sum Aggregation Entries:\")\n",
    "print(top20_sum.head())\n",
    "\n",
    "print(\"\\nSample Bottom 80% Sum Aggregation Entries:\")\n",
    "print(bottom80_sum.head())\n",
    "\n",
    "print(\"\\nSample Top 20% Mean Aggregation Entries:\")\n",
    "print(top20_mean.head())\n",
    "\n",
    "print(\"\\nSample Bottom 80% Mean Aggregation Entries:\")\n",
    "print(bottom80_mean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pipeline_time_distribution(combined_df, step_name):\n",
    "    step_df = combined_df[combined_df['data.name'] == step_name]\n",
    "    \n",
    "    if step_df.empty:\n",
    "        print(f\"No data found for step: {step_name}\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(step_df['data.reading'], bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of Pipeline Times for {step_name}\")\n",
    "    plt.xlabel(\"Pipeline Time (seconds)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "pipeline_time_distribution(df, 'TRIP_SEGMENTATION/segment_into_trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_time_trends(combined_df, step_name, freq='D'):\n",
    "    \"\"\"\n",
    "    Plots the trend of pipeline times over time for a specific step.\n",
    "    :param freq: Resampling frequency ('D' for daily, 'W' for weekly, 'M' for monthly)\n",
    "    \"\"\"\n",
    "    step_df = combined_df[combined_df['data.name'] == step_name].copy()\n",
    "    \n",
    "    if step_df.empty:\n",
    "        print(f\"No data found for step: {step_name}\")\n",
    "        return\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    step_df['datetime'] = pd.to_datetime(step_df['metadata.write_ts'], unit='s')\n",
    "    \n",
    "    # Set datetime as index\n",
    "    step_df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # Resample and calculate mean pipeline time\n",
    "    resampled = step_df['data.reading'].resample(freq).mean()\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12,6))\n",
    "    resampled.plot()\n",
    "    plt.title(f\"Trend of Pipeline Times Over Time for {step_name}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Average Pipeline Time (seconds)\")\n",
    "    plt.show()\n",
    "\n",
    "pipeline_time_trends(df, 'TRIP_SEGMENTATION', 'W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bottlenecks(combined_df, top_n=5):\n",
    "    avg_time_df = combined_df.groupby('data.name')['data.reading'].mean().reset_index()\n",
    "    avg_time_df.rename(columns={'data.reading': 'average_time'}, inplace=True)\n",
    "    \n",
    "    bottlenecks = avg_time_df.sort_values(by='average_time', ascending=False).head(top_n)\n",
    "    \n",
    "    print(f\"\\nTop {top_n} Bottleneck Pipeline Steps:\")\n",
    "    print(bottlenecks)\n",
    "    \n",
    "    # Optionally, visualize\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x='average_time', y='data.name', data=bottlenecks, palette='viridis')\n",
    "    plt.title(f\"Top {top_n} Bottleneck Pipeline Steps by Average Time\")\n",
    "    plt.xlabel(\"Average Pipeline Time (seconds)\")\n",
    "    plt.ylabel(\"Pipeline Step\")\n",
    "    plt.show()\n",
    "\n",
    "identify_bottlenecks(function_level_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_pipeline_times(combined_df, step_name):\n",
    "    step_df = combined_df[combined_df['data.name'] == step_name].copy()\n",
    "    \n",
    "    if step_df.empty:\n",
    "        print(f\"No data found for step: {step_name}\")\n",
    "        return\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    step_df['datetime'] = pd.to_datetime(step_df['metadata.write_ts'], unit='s')\n",
    "    \n",
    "    # Extract hour and day of week\n",
    "    step_df['hour'] = step_df['datetime'].dt.hour\n",
    "    step_df['day_of_week'] = step_df['datetime'].dt.day_name()\n",
    "    \n",
    "    # Create pivot table\n",
    "    pivot = step_df.pivot_table(values='data.reading', index='day_of_week', columns='hour', aggfunc='mean')\n",
    "    \n",
    "    # Reorder days of the week\n",
    "    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    pivot = pivot.reindex(days_order)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(15,7))\n",
    "    sns.heatmap(pivot, cmap='YlGnBu', annot=True, fmt=\".2f\")\n",
    "    plt.title(f\"Heatmap of Average Pipeline Times for {step_name}\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Day of Week\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "heatmap_pipeline_times(df, 'TRIP_SEGMENTATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "def define_pipeline_executions(df):\n",
    "    grouped = df.groupby(['user_id', 'metadata.key'])\n",
    "    pipeline_executions = []\n",
    "    \n",
    "    for (user_id, key), group in grouped:\n",
    "        execution = {\n",
    "            'user_id': user_id,\n",
    "            'pipeline_key': key,\n",
    "            'start_time': group['metadata.write_ts'].min(),\n",
    "            'end_time': group['metadata.write_ts'].max(),\n",
    "            'total_time': group['data.reading'].sum(),\n",
    "            'num_steps': group.shape[0],\n",
    "            'steps': list(group.sort_values('metadata.write_ts')['data.name']),\n",
    "            'steps_reading': list(group.sort_values('metadata.write_ts')['data.reading'])\n",
    "        }\n",
    "        pipeline_executions.append(execution)\n",
    "    \n",
    "    executions_df = pd.DataFrame(pipeline_executions)\n",
    "    return executions_df\n",
    "\n",
    "\n",
    "# combined_pipeline_df is  DataFrame with all users' data\n",
    "executions_df = define_pipeline_executions(df)\n",
    "print(\"Pipeline Executions Defined:\")\n",
    "print(executions_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_time_variability_per_step(df):\n",
    "    variability_df = df.groupby('data.name')['data.reading'].agg(['mean', 'std', 'var']).reset_index()\n",
    "    variability_df.rename(columns={'mean': 'average_time_sec', 'std': 'std_dev_sec', 'var': 'variance_sec2'}, inplace=True)\n",
    "    \n",
    "    # Sort by standard deviation descending\n",
    "    variability_df = variability_df.sort_values(by='std_dev_sec', ascending=False)\n",
    "    \n",
    "    print(\"\\nExecution Time Variability per Pipeline Step:\")\n",
    "    print(variability_df)\n",
    "    \n",
    "    # Visualization: Box Plots to visualize variability\n",
    "    plt.figure(figsize=(14,10))\n",
    "    sns.boxplot(x='data.reading', y='data.name', data=df, palette='coolwarm')\n",
    "    plt.title(\"Execution Time Variability per Pipeline Step\")\n",
    "    plt.xlabel(\"Execution Time (seconds)\")\n",
    "    plt.ylabel(\"Pipeline Step\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save to CSV\n",
    "    variability_df.to_csv('execution_time_variability_per_step.csv', index=False)\n",
    "    print(\"Saved execution time variability to 'execution_time_variability_per_step.csv'\")\n",
    "\n",
    "\n",
    "execution_time_variability_per_step(function_level_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
